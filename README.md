# ğŸŒ¦ï¸ Full-Stack On-Premise Data Pipeline for IoT & Weather Analytics

## ğŸ“Œ Project Overview

This project simulates a real-time weather analytics platform using open-source tools. It ingests weather data from public APIs and synthetic sources, processes it using Apache Spark, stores it in Hive and MySQL, and orchestrates everything with Apache Airflow. Monitoring is enabled via Grafana and Prometheus. The system is designed for on-premise deployment and optionally containerized using Docker Compose.

---

## ğŸ§± Folder Structure

---

weather_pipeline_project/ â”œâ”€â”€ airflow/ â”‚ â”œâ”€â”€ airflow.cfg â”‚ â”œâ”€â”€ airflow.db â”‚ â”œâ”€â”€ dags/ â”‚ â”‚ â”œâ”€â”€ weather_to_kafka_dag.py â”‚ â”‚ â”œâ”€â”€ faker_ingestion_dag.py â”‚ â”‚ â”œâ”€â”€ batch_etl_dag.py â”‚ â”‚ â””â”€â”€ connectivity_check_dag.py â”œâ”€â”€ kafka/ â”‚ â””â”€â”€ weather_to_kafka.py â”œâ”€â”€ faker/ â”‚ â””â”€â”€ faker_ingestion.py â”œâ”€â”€ spark/ â”‚ â”œâ”€â”€ spark_streaming.py â”‚ â”œâ”€â”€ spark_batch_etl.py â”‚ â””â”€â”€ last_etl_timestamp.txt â”œâ”€â”€ hive/ â”‚ â”œâ”€â”€ create_final_table.sql â”‚ â””â”€â”€ apache-hive-3.1.3-bin/ â”œâ”€â”€ hadoop/ â”‚ â””â”€â”€ hadoop-3.3.6/ â”œâ”€â”€ csv_parquet_storage/ â”‚ â”œâ”€â”€ csv_output/ â”‚ â”œâ”€â”€ parquet_output/ â”‚ â”œâ”€â”€ hive_final_table_export/ â”‚ â””â”€â”€ checkpoint/ â”œâ”€â”€ spark-events/ â”‚ â””â”€â”€ [Spark application logs] â”œâ”€â”€ kafka-logs/ â”‚ â””â”€â”€ [Kafka broker logs] â”œâ”€â”€ grafana/ â”‚ â””â”€â”€ [Grafana setup files] â”œâ”€â”€ zookeeper-data/ â”‚ â””â”€â”€ [Zookeeper metadata]

---
---

## âš™ï¸ Technologies Used

| Tool               | Purpose                          |
|--------------------|----------------------------------|
| Python + Faker     | Synthetic data generation        |
| Apache Kafka       | Real-time ingestion              |
| Apache Spark       | Streaming + Batch ETL            |
| Apache Hive        | Data lake storage                |
| MySQL              | Relational storage               |
| Apache Airflow     | Workflow orchestration           |
| Grafana + Prometheus | Monitoring (optional)         |
| Docker Compose     | Containerized deployment (optional)

---

## ğŸš€ Pipeline Components

### ğŸ”¹ 1. Ingestion

- **Weather API to Kafka**  
  - Script: `kafka/weather_to_kafka.py`  
  - DAG: `weather_to_kafka_dag.py`  
  - Pulls weather data from OpenWeatherMap every minute and pushes to Kafka topic `weather-topic`.

- **Faker to CSV + MySQL**  
  - Script: `faker/faker_ingestion.py`  
  - DAG: `faker_ingestion_dag.py`  
  - Generates synthetic weather logs and inserts mock sensor data into MySQL every minute.

---

### ğŸ”¹ 2. Processing

- **Spark Streaming**  
  - Script: `spark/spark_streaming.py`  
  - Managed via `systemd` (not Airflow)  
  - Consumes Kafka topic and writes Parquet files every 5 minutes to `csv_parquet_storage/parquet_output`.

- **Spark Batch ETL**  
  - Script: `spark/spark_batch_etl.py`  
  - DAG: `batch_etl_dag.py`  
  - Joins new CSV files + MySQL sensor data, transforms, and loads into Hive and final MySQL table.

---

### ğŸ”¹ 3. Storage

- **Parquet Files**  
  - Stored in `csv_parquet_storage/parquet_output/` from Spark Streaming.

- **Hive Table**  
  - Table: `final_table`  
  - Created via `hive/create_final_table.sql`

- **MySQL Final Table**  
  - Table: `final_table` in `weather_db`  
  - Populated via Spark Batch ETL

- **CSV Export from Hive**  
  - Output: `csv_parquet_storage/hive_final_table_export/`

---

### ğŸ”¹ 4. Orchestration

- **Apache Airflow**  
  - DAGs scheduled for ingestion and batch ETL
  - Uses `LocalExecutor` with MySQL backend
  - DAGs are modular and non-overlapping (`max_active_runs=1`)

---

## ğŸ§ª Running the Project

### Prerequisites

- Python 3.8+
- Kafka, Spark, Hive, MySQL installed and configured
- Airflow initialized with MySQL backend
- Optional: Docker Compose setup

### Steps

1. Start Kafka, MySQL, Hive, Spark services
2. Initialize Airflow:
   ```bash
   airflow db init
   airflow users create --role Admin --username gowtham ...

Unpause DAGs:

airflow dags unpause weather_to_kafka
airflow dags unpause faker_ingestion_dag
airflow dags unpause batch_etl_dag

Start Spark Streaming via systemd:

sudo systemctl start spark-streaming.service

Monitor DAGs via Airflow UI (http://<host>:5050)

Validate Kafka topic, Parquet files, Hive tables, and MySQL final table.
